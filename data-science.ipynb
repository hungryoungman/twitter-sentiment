{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fe3da547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as ml\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "86ad621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  \n",
       "3  793124635873275904  \n",
       "4  793125156185137153  "
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This dataset aggregates tweets pertaining to climate change collected \n",
    "between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were annotated. \n",
    "Each tweet is labelled independently by 3 reviewers. \n",
    "This dataset only contains tweets that all 3 reviewers agreed on (the rest were discarded).\n",
    "\n",
    "Each tweet is labelled as one of the following classes:\n",
    "\n",
    "    2(News): the tweet links to factual news about climate change\n",
    "    1(Pro): the tweet supports the belief of man-made climate change\n",
    "    0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "    -1(Anti): the tweet does not believe in man-made climate change\n",
    "\n",
    "'''\n",
    "\n",
    "data = pd.read_csv(\"./twitter_sentiment_data.csv\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "2a8fef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's begin!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data statistics:\n",
    "0. remove chars not in 0-9,a-z,A-Z  [done]\n",
    "1. total tweets. [done]\n",
    "2. sentiment -\n",
    "    a. tweets per sentiment [done]\n",
    "    b. top-n words occuring per sentiment [done]\n",
    "3. #tags - \n",
    "    a. top-n used function [done]\n",
    "    b. total distinct tags used [done]\n",
    "4. @therate -\n",
    "    a. top-n used function [done]\n",
    "    b. total distinct @therate used [done] \n",
    "    \n",
    "    \n",
    "data analysis:\n",
    "0. partition the data set into train & test. (not using verify here.)[done]\n",
    "1. create word vector of top-100 words used. [done]\n",
    "2. train ml models on that word vector only. [done]\n",
    "'''\n",
    "\n",
    "print(\"Let's begin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c9fe5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def remove_no_chars(self, text):\n",
    "        return re.sub(r'[^a-zA-Z0-9@#]+',' ', text)\n",
    "    \n",
    "    def remove_links(self, text):\n",
    "        return re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    def to_lower(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        text = text.split()\n",
    "        return ' '.join([word for word in text if word not in stopwords.words('english')])\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        text = text.split()\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "    \n",
    "class Pipeline:\n",
    "    def pipe(self, *funcs):\n",
    "        self.funcs = funcs\n",
    "    \n",
    "    def call(self, text):\n",
    "        for func in self.funcs:\n",
    "            text = func(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "4efc2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sort_dict(self, wordmap):\n",
    "        keys = list(wordmap.keys())\n",
    "        values = list(wordmap.values())\n",
    "        sorted_value_index = np.argsort(values)[::-1] #reversing the list here.\n",
    "        sorted_wordmap = {keys[i]: values[i] for i in sorted_value_index}\n",
    "        return sorted_wordmap\n",
    "\n",
    "class Stats:\n",
    "    def __init__(self, data):\n",
    "        self._data = data #underscore makes it a private member\n",
    "        self._util = Utility()\n",
    "        \n",
    "        cleaner = DataCleaner()\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.pipe( cleaner.remove_links, cleaner.remove_no_chars, cleaner.to_lower,\n",
    "                      cleaner.remove_stopwords, cleaner.lemmatize)\n",
    "        \n",
    "        self._data.message = self._data.message.apply(pipeline.call)\n",
    "        self._sentiments = set(self._data.sentiment)\n",
    "        \n",
    "    def plot_tweets_per_sentiment(self):\n",
    "        self._data.sentiment.value_counts().plot(kind='barh')\n",
    "        print(\"Total tweets:\", len(self._data))\n",
    "        \n",
    "    def get_wordmap_by_condition(self, condition):\n",
    "        def count_words(text, senti): #inner func to use s2w_map as global-external variable\n",
    "            for word in text.split(' '):\n",
    "                if condition(word): #condition-lambda\n",
    "                    if word in s2w_map[senti]:\n",
    "                        s2w_map[senti][word] += 1\n",
    "                    else:\n",
    "                        s2w_map[senti][word] = 1\n",
    "        \n",
    "        s2w_map = {} #dict of dicts.\n",
    "        for i in self._sentiments:\n",
    "            s2w_map[i] = {}\n",
    "            \n",
    "        self._data.apply(lambda x: count_words(x.message, x.sentiment), axis=1)\n",
    "        \n",
    "        for i in self._sentiments:\n",
    "            s2w_map[i] = self._util.sort_dict(s2w_map[i])\n",
    "        return s2w_map\n",
    "        \n",
    "    def get_topN_words_per_sentiment(self, topN):\n",
    "        y = self.get_wordmap_by_condition(lambda x: len(x) > 4) #word length > 4\n",
    "        for i in self._sentiments:\n",
    "            print(i, \"\\n\", list(y[i].items())[:min(topN, len(y[i].items()))])\n",
    "            print(\"*******************\")\n",
    "        \n",
    "    def get_topN_tags_per_sentiment(self, topN):\n",
    "        y = self.get_wordmap_by_condition(lambda x: '#' in x and len(x) > 1) #word length > 4\n",
    "        for i in self._sentiments:\n",
    "            print(i, \"\\n\", list(y[i].items())[:min(topN, len(y[i].items()))])\n",
    "            print(\"*******************\")\n",
    "    \n",
    "    def get_topN_atTheRates_per_sentiment(self, topN):\n",
    "        y = self.get_wordmap_by_condition(lambda x: '@' in x and len(x) > 1) #word length > 4\n",
    "        for i in self._sentiments:\n",
    "            print(i, \"\\n\", list(y[i].items())[:min(topN, len(y[i].items()))])\n",
    "            print(\"*******************\")\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self._data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "7f7d50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = Stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b40c44b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 43943\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stat.plot_tweets_per_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "aee8ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " [('@realdonaldtrump', 84), ('@cnn', 32), ('@civiljustus', 32), ('@youtube', 28), ('@ultravlolence', 23), ('@foxnews', 20), ('@potus', 20), ('@nytimes', 19), ('@kurteichenwald', 18), ('@jay', 18)]\n",
      "*******************\n",
      "1 \n",
      " [('@stephenschlegel', 507), ('@sensanders', 445), ('@realdonaldtrump', 377), ('@leodicaprio', 255), ('@berniesanders', 245), ('@natgeochannel', 219), ('@sethmacfarlane', 139), ('@climatecentral', 112), ('@climatereality', 111), ('@cnn', 110)]\n",
      "*******************\n",
      "2 \n",
      " [('@thehill', 213), ('@nytimes', 113), ('@cnn', 105), ('@reuters', 105), ('@independent', 92), ('@guardian', 88), ('@washingtonpost', 88), ('@guardianeco', 63), ('@worldfnature', 62), ('@sciam', 55)]\n",
      "*******************\n",
      "-1 \n",
      " [('@realdonaldtrump', 173), ('@stevesgoddard', 135), ('@tan123', 41), ('@prisonplanet', 36), ('@algore', 35), ('@junkscience', 31), ('@foxnews', 31), ('@climatedepot', 26), ('@carbongate', 24), ('@sensanders', 24)]\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "stat.get_topN_atTheRates_per_sentiment(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fd4bb951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " [('#climate', 59), ('#climatechange', 52), ('#trump', 20), ('#globalwarming', 18), ('#debatenight', 16), ('#cop21', 14), ('#auspol', 14), ('#cop22', 12), ('#science', 9), ('#qanda', 9)]\n",
      "*******************\n",
      "1 \n",
      " [('#climate', 547), ('#climatechange', 399), ('#beforetheflood', 268), ('#actonclimate', 157), ('#parisagreement', 108), ('#cop22', 98), ('#imvotingbecause', 93), ('#auspol', 76), ('#globalwarming', 74), ('#trump', 72)]\n",
      "*******************\n",
      "2 \n",
      " [('#climate', 333), ('#climatechange', 177), ('#news', 81), ('#environment', 78), ('#trump', 50), ('#science', 42), ('#globalwarming', 32), ('#cop22', 27), ('#p2', 22), ('#cop21', 21)]\n",
      "*******************\n",
      "-1 \n",
      " [('#tcot', 65), ('#maga', 31), ('#climate', 27), ('#climatechange', 25), ('#pjnet', 23), ('#trump', 21), ('#globalwarming', 19), ('#fakenews', 15), ('#p2', 13), ('#teaparty', 11)]\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "stat.get_topN_tags_per_sentiment(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "08972e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " [('climate', 4601), ('change', 4572), ('global', 3240), ('warming', 3147), ('about', 941), ('trump', 459), ('believe', 227), ('think', 207), ('people', 204), ('there', 193)]\n",
      "*******************\n",
      "1 \n",
      " [('change', 19389), ('climate', 19278), ('global', 4295), ('warming', 3932), ('about', 2873), ('trump', 2283), ('believe', 1927), ('doesn', 1435), ('world', 1186), ('people', 1140)]\n",
      "*******************\n",
      "2 \n",
      " [('change', 8188), ('climate', 8106), ('trump', 2037), ('global', 1414), ('warming', 1232), ('scientists', 481), ('fight', 399), ('study', 397), ('about', 393), ('world', 370)]\n",
      "*******************\n",
      "-1 \n",
      " [('climate', 2382), ('change', 2255), ('global', 1972), ('warming', 1912), ('about', 421), ('science', 191), ('obama', 186), ('there', 184), ('@realdonaldtrump', 173), ('people', 168)]\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "stat.get_topN_words_per_sentiment(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "fcf1ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MachineLearn:\n",
    "    def __init__(self, data):\n",
    "        self.X = data['message']\n",
    "        self.y = data['sentiment']\n",
    "        \n",
    "    def split_data(self, ratio): #ratio = 0.30\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "        train_test_split(self.X, self.y, test_size=ratio, random_state=123)\n",
    "        \n",
    "        print('Training Data :', self.X_train.shape)\n",
    "        print('Testing Data : ', self.X_test.shape)\n",
    "        \n",
    "    def extract_feature(self, X, kind='count'):\n",
    "        if kind == 'count':\n",
    "            self.transformer = CountVectorizer()\n",
    "            tranformed_data = self.transformer.fit_transform(X)\n",
    "            return tranformed_data\n",
    "        \n",
    "        if kind == 'tfidf':\n",
    "            self.transformer = TfidfVectorizer()\n",
    "            tranformed_data = self.transformer.fit_transform(X)\n",
    "            return tranformed_data\n",
    "        \n",
    "    def applyModel(self, ratio = 0.30, model_type='lr', kind='count'):\n",
    "        #split into train-test.\n",
    "        self.split_data(ratio)\n",
    "        \n",
    "        #transform train and test.\n",
    "        Xtrain = self.extract_feature(self.X_train, kind)\n",
    "        Xtest = self.transformer.transform(self.X_test)\n",
    "        \n",
    "        #apply model\n",
    "        if model_type == 'lr':\n",
    "            model = LogisticRegression()\n",
    "        if model_type == 'rf':\n",
    "            model = RandomForestRegressor(n_estimators = 100, max_depth=5, random_state = 0)\n",
    "        \n",
    "        #fit train data\n",
    "        model.fit(Xtrain, self.y_train)\n",
    "        self.ypred = model.predict(Xtest)\n",
    "        \n",
    "        print(\"Model:\", model_type)\n",
    "        \n",
    "        if model_type == 'lr':\n",
    "            print(classification_report(self.y_test, self.ypred))\n",
    "            print(\"Accuracy score: \", accuracy_score(self.y_test,  self.ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "410113f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (30760,)\n",
      "Testing Data :  (13183,)\n",
      "Model: rf\n"
     ]
    }
   ],
   "source": [
    "machineLearning = MachineLearn(data)\n",
    "machineLearning.applyModel(model_type='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550abdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fc88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
